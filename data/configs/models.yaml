models:
  # OpenAI Models
  gpt-4-turbo:
    model_id: gpt-4-turbo
    name: GPT-4 Turbo
    provider: openai
    model_type: chat
    version: "gpt-4-1106-preview"
    api_key_env: OPENAI_API_KEY
    
    capabilities:
      reasoning_ability: 0.95
      creative_ability: 0.90
      code_ability: 0.85
      analysis_ability: 0.95
      factual_accuracy: 0.90
      instruction_following: 0.95
      max_context_length: 128000
      max_output_length: 4096
      supported_tasks: [reasoning, creative, code, analysis, qa, summarization]
      domain_expertise:
        technical: 0.9
        scientific: 0.85
        business: 0.8
        creative: 0.9
        general: 0.95
      supported_languages: [en, es, fr, de, it, pt, ru, ja, ko, zh]
      supports_function_calling: true
      supports_json_mode: true
      supports_vision: false
      
    constraints:
      input_cost_per_1k_tokens: 0.01
      output_cost_per_1k_tokens: 0.03
      avg_latency_ms: 2000
      p95_latency_ms: 5000
      availability: 0.99
      requests_per_minute: 500
      tokens_per_minute: 150000
      avg_quality_score: 0.92
      user_satisfaction: 0.94
      
    preferred_for_tasks: [reasoning, analysis, creative]
    avoid_for_tasks: [simple_qa]
    is_enabled: true

  gpt-3.5-turbo:
    model_id: gpt-3.5-turbo
    name: GPT-3.5 Turbo
    provider: openai
    model_type: chat
    version: "gpt-3.5-turbo"
    api_key_env: OPENAI_API_KEY
    
    capabilities:
      reasoning_ability: 0.75
      creative_ability: 0.70
      code_ability: 0.75
      analysis_ability: 0.75
      factual_accuracy: 0.85
      instruction_following: 0.85
      max_context_length: 16385
      max_output_length: 4096
      supported_tasks: [qa, conversation, summarization, translation, simple_code]
      domain_expertise:
        technical: 0.7
        scientific: 0.65
        business: 0.7
        creative: 0.7
        general: 0.85
      supported_languages: [en, es, fr, de, it, pt, ru, ja, ko, zh]
      supports_function_calling: true
      supports_json_mode: true
      
    constraints:
      input_cost_per_1k_tokens: 0.0005
      output_cost_per_1k_tokens: 0.0015
      avg_latency_ms: 800
      p95_latency_ms: 2000
      availability: 0.995
      requests_per_minute: 3500
      tokens_per_minute: 250000
      avg_quality_score: 0.82
      user_satisfaction: 0.85
      
    preferred_for_tasks: [qa, conversation, summarization]
    is_enabled: true

  # Anthropic Models
  claude-3-sonnet:
    model_id: claude-3-sonnet
    name: Claude 3 Sonnet
    provider: anthropic
    model_type: chat
    version: "claude-3-sonnet-20240229"
    api_key_env: ANTHROPIC_API_KEY
    
    capabilities:
      reasoning_ability: 0.90
      creative_ability: 0.95
      code_ability: 0.80
      analysis_ability: 0.90
      factual_accuracy: 0.92
      instruction_following: 0.93
      max_context_length: 200000
      max_output_length: 4096
      supported_tasks: [reasoning, creative, analysis, qa, conversation]
      domain_expertise:
        technical: 0.8
        scientific: 0.85
        business: 0.8
        creative: 0.95
        general: 0.9
      supported_languages: [en, es, fr, de, it, pt, ru, ja, ko, zh]
      supports_function_calling: false
      supports_json_mode: false
      
    constraints:
      input_cost_per_1k_tokens: 0.003
      output_cost_per_1k_tokens: 0.015
      avg_latency_ms: 1500
      p95_latency_ms: 4000
      availability: 0.98
      requests_per_minute: 1000
      tokens_per_minute: 100000
      avg_quality_score: 0.89
      user_satisfaction: 0.91
      
    preferred_for_tasks: [creative, analysis, reasoning]
    is_enabled: true

  claude-3-haiku:
    model_id: claude-3-haiku
    name: Claude 3 Haiku
    provider: anthropic
    model_type: chat
    version: "claude-3-haiku-20240307"
    api_key_env: ANTHROPIC_API_KEY
    
    capabilities:
      reasoning_ability: 0.70
      creative_ability: 0.75
      code_ability: 0.65
      analysis_ability: 0.70
      factual_accuracy: 0.85
      instruction_following: 0.85
      max_context_length: 200000
      max_output_length: 4096
      supported_tasks: [qa, conversation, summarization, simple_analysis]
      domain_expertise:
        technical: 0.6
        scientific: 0.65
        business: 0.7
        creative: 0.8
        general: 0.8
      supported_languages: [en, es, fr, de, it, pt, ru, ja, ko, zh]
      
    constraints:
      input_cost_per_1k_tokens: 0.00025
      output_cost_per_1k_tokens: 0.00125
      avg_latency_ms: 500
      p95_latency_ms: 1200
      availability: 0.995
      requests_per_minute: 2000
      tokens_per_minute: 200000
      avg_quality_score: 0.78
      user_satisfaction: 0.82
      
    preferred_for_tasks: [qa, conversation, summarization]
    is_enabled: true

  # Ollama Local Models
  llama3-8b:
    model_id: llama3-8b
    name: Llama 3 8B
    provider: ollama
    model_type: chat
    version: "llama3:8b"
    endpoint_url: "http://localhost:11434"
    
    capabilities:
      reasoning_ability: 0.65
      creative_ability: 0.60
      code_ability: 0.55
      analysis_ability: 0.60
      factual_accuracy: 0.75
      instruction_following: 0.80
      max_context_length: 8192
      max_output_length: 2048
      supported_tasks: [qa, conversation, simple_summarization]
      domain_expertise:
        technical: 0.5
        scientific: 0.5
        business: 0.6
        creative: 0.6
        general: 0.75
      supported_languages: [en, es, fr, de, it, pt]
      
    constraints:
      input_cost_per_1k_tokens: 0.0  # Local model
      output_cost_per_1k_tokens: 0.0
      avg_latency_ms: 200
      p95_latency_ms: 500
      availability: 0.95  # Depends on local setup
      avg_quality_score: 0.72
      user_satisfaction: 0.75
      
    preferred_for_tasks: [qa, conversation]
    avoid_for_tasks: [complex_reasoning, expert_analysis]
    is_enabled: true
    health_check_url: "http://localhost:11434/api/version"

  codellama-13b:
    model_id: codellama-13b
    name: CodeLlama 13B
    provider: ollama
    model_type: code
    version: "codellama:13b"
    endpoint_url: "http://localhost:11434"
    
    capabilities:
      reasoning_ability: 0.70
      creative_ability: 0.40
      code_ability: 0.85
      analysis_ability: 0.65
      factual_accuracy: 0.80
      instruction_following: 0.85
      max_context_length: 16384
      max_output_length: 4096
      supported_tasks: [code, code_review, debugging, technical_analysis]
      domain_expertise:
        technical: 0.9
        scientific: 0.6
        business: 0.4
        creative: 0.3
        general: 0.6
      supported_languages: [en]
      supports_code_execution: false
      
    constraints:
      input_cost_per_1k_tokens: 0.0
      output_cost_per_1k_tokens: 0.0
      avg_latency_ms: 400
      p95_latency_ms: 1000
      availability: 0.95
      avg_quality_score: 0.82
      user_satisfaction: 0.85
      
    preferred_for_tasks: [code, debugging, code_review]
    avoid_for_tasks: [creative, general_conversation]
    is_enabled: true
    health_check_url: "http://localhost:11434/api/version"

# Global configuration
global_config:
  default_model: gpt-3.5-turbo
  fallback_model: llama3-8b
  
  # Default weights for routing
  default_weights:
    capability_weight: 0.4
    cost_weight: 0.2
    latency_weight: 0.2
    quality_weight: 0.2
  
  # Global constraints
  max_cost_per_request: 0.10
  max_latency_ms: 5000
  min_quality_threshold: 0.7
  
  # RAG configuration
  rag_config:
    embedding_model: "all-MiniLM-L6-v2"
    max_results: 5
    similarity_threshold: 0.7
    chunk_size: 512
    chunk_overlap: 50
